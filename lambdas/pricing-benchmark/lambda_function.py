"""
Ride-Hailing Pricing Benchmark Lambda Function

This Lambda function implements a comprehensive experimental environment for ride-hailing pricing.
Loading real taxi data from S3 parquet files and implementing 4 pricing methods:
1. MinMaxCostFlow - Min-cost flow algorithm with capacity scaling
2. MAPS - Area-based pricing with bipartite matching
3. LinUCB - Contextual bandit learning with Upper Confidence Bound
4. LP - Gupta-Nagarajan Linear Program optimization
"""

import json
import os
import sys
import logging
import traceback
import time
import random
import math
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import io

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_imports():
    """Test all critical imports for debugging."""
    test_results = {}
    
    try:
        import boto3
        test_results['boto3'] = f"âœ… {boto3.__version__}"
    except Exception as e:
        test_results['boto3'] = f"âŒ {e}"
    
    try:
        import numpy as np
        import pandas as pd
        import networkx as nx
        import pulp
        import pyarrow.parquet as pq
        test_results['scientific_packages'] = "âœ… All scientific packages available"
    except Exception as e:
        test_results['scientific_packages'] = f"âŒ {e}"
    
    return test_results

try:
    import boto3
    import pandas as pd
    import numpy as np
    import networkx as nx
    import pulp as pl
    from scipy.spatial import distance_matrix
    import pickle
    import io
    import pyarrow as pa
    import pyarrow.parquet as pq
    
    # Optional: pyarrow for parquet files
    try:
        import pyarrow.parquet as pq
        import pyarrow as pa
        PARQUET_SUPPORT = True
    except ImportError:
        logger.warning("âš ï¸ PyArrow not available - parquet files will not be supported")
        PARQUET_SUPPORT = False
    
    IMPORTS_SUCCESSFUL = True
    logger.info("âœ… All critical imports successful")
except Exception as e:
    logger.error(f"âŒ Import error: {e}")
    IMPORTS_SUCCESSFUL = False
    PARQUET_SUPPORT = False


class PricingExperimentRunner:
    """Complete implementation of ride-hailing pricing experiments."""
    
    def __init__(self, num_eval: int = 1000):
        self.s3_client = boto3.client('s3')
        self.bucket_name = os.environ.get('S3_BUCKET', 'magisterka')
        
        # Hikima parameters (from original paper)
        self.epsilon = 1e-10
        self.alpha = 18.0
        self.s_taxi = 25.0
        self.num_eval = num_eval  # Configurable via event parameters (default: 1000 as per Hikima)
        
        # Acceptance function parameters
        self.beta = 1.3
        self.gamma = (0.3 * np.sqrt(3) / np.pi).astype(np.float64)
        
        # LinUCB parameters
        self.ucb_alpha = 0.5
        self.base_price = 5.875
        self.arm_price_multipliers = np.array([0.6, 0.8, 1.0, 1.2, 1.4])
        self.arm_prices = self.base_price * self.arm_price_multipliers
        
        # Cache for area information and distance matrix
        self.area_info = None
        self.distance_matrix = None
        
        logger.info("ðŸ”§ Initialized PricingExperimentRunner")
    
    def safe_int_convert(self, value):
        """Safely convert PyArrow Timestamp or other objects to int."""
        try:
            if hasattr(value, 'value'):  # PyArrow Timestamp has .value attribute
                return int(value.value)
            elif hasattr(value, 'timestamp'):  # Pandas Timestamp
                return int(value.timestamp())
            else:
                return int(value)
        except (ValueError, TypeError, AttributeError):
            # Fallback: try to extract numeric value
            try:
                return int(float(str(value)))
            except:
                return 1  # Default location ID if all else fails
    
    def load_experiment_config(self, config_name: str) -> Dict[str, Any]:
        """Load experiment configuration from S3."""
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name, 
                Key=f"configs/{config_name}"
            )
            config = json.loads(response['Body'].read().decode('utf-8'))
            logger.info(f"âœ… Loaded config: {config_name}")
            return config
        except Exception as e:
            logger.error(f"âŒ Failed to load config {config_name}: {e}")
            # Return default config
            return {
                "hikima_parameters": {
                    "epsilon": 1e-10,
                    "alpha": 18.0,
                    "s_taxi": 25.0,
                    "num_eval": 1000
                }
            }
    
    def load_area_information(self) -> pd.DataFrame:
        """Load area information from S3 if not cached."""
        if self.area_info is None:
            try:
                # Try to load area information from S3
                response = self.s3_client.get_object(
                    Bucket=self.bucket_name,
                    Key="reference_data/area_info.csv"
                )
                self.area_info = pd.read_csv(io.BytesIO(response['Body'].read()))
                logger.info(f"âœ… Loaded area information: {len(self.area_info)} areas")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not load area_information.csv: {e}")
                # Create minimal area info for NYC taxi zones (1-263)
                self.area_info = pd.DataFrame({
                    'LocationID': range(1, 264),
                    'borough': ['Manhattan'] * 100 + ['Brooklyn'] * 80 + ['Queens'] * 83,
                    'latitude': np.random.uniform(40.7, 40.8, 263),
                    'longitude': np.random.uniform(-74.02, -73.93, 263)
                })
        return self.area_info
    
    def load_taxi_data(self, taxi_type: str, year: int, month: int, day: int, 
                      borough: str, time_start: datetime, time_end: datetime) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load SINGLE taxi type data from S3 - SEPARATE experiments per vehicle type (NO combination)."""
        
        df = None
        
        # Load ONLY the specified taxi type (NO combination)
        try:
            if PARQUET_SUPPORT:
                s3_key = f"datasets/{taxi_type}/year={year}/month={month:02d}/{taxi_type}_tripdata_{year}-{month:02d}.parquet"
                logger.info(f"ðŸ“¥ Loading {taxi_type.upper()} data: s3://{self.bucket_name}/{s3_key}")
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
                table = pq.read_table(io.BytesIO(response['Body'].read()))
                df = table.to_pandas()
            else:
                raise Exception("No parquet support")
        except Exception as e:
            try:
                s3_key = f"datasets/{taxi_type}/year={year}/month={month:02d}/{taxi_type}_tripdata_{year}-{month:02d}.csv"
                logger.info(f"ðŸ“¥ Fallback CSV: {taxi_type.upper()} data: s3://{self.bucket_name}/{s3_key}")
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
                df = pd.read_csv(io.BytesIO(response['Body'].read()))
            except Exception as e2:
                logger.warning(f"âš ï¸ Could not load {taxi_type} taxi data: {e2}")
        
        # If loading failed, use fallback
        if df is None:
            logger.warning(f"âš ï¸ No {taxi_type} taxi data available - using synthetic fallback")
            return self.generate_fallback_data(taxi_type, year, month, day, borough, time_start, time_end)
        
        logger.info(f"ðŸ“Š Loaded {taxi_type.upper()}: {len(df)} trips")
        
        # Process the loaded data using exact Hikima methodology for SINGLE taxi type
        return self.process_taxi_data_hikima_single_type(df, taxi_type, year, month, day, borough, time_start, time_end)
    
    def generate_fallback_data(self, taxi_type: str, year: int, month: int, day: int, 
                              borough: str, time_start: datetime, time_end: datetime) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Generate realistic data following research statistics when real data is not available."""
        logger.info(f"ðŸŽ² Generating Hikima-style data for {taxi_type} taxis in {borough}")
        
        # Use time-based seed for different results per scenario
        scenario_seed = int(time_start.timestamp()) % 10000
        np.random.seed(scenario_seed)
        
        # Use Hikima statistics from Table 1 for realistic numbers
        if borough == 'Manhattan':
            if day in [6]:  # Holiday (10/6 in paper)
                n_requesters = int(np.random.normal(80.0, 11.1))
                n_taxis = int(np.random.normal(76.1, 13.1))
            else:  # Weekday (10/10 in paper)
                n_requesters = int(np.random.normal(100.1, 17.4))
                n_taxis = int(np.random.normal(96.6, 21.6))
        elif borough == 'Queens':
            if day in [6]:
                n_requesters = int(np.random.normal(92.3, 22.7))
                n_taxis = int(np.random.normal(88.3, 27.0))
            else:
                n_requesters = int(np.random.normal(95.4, 23.1))
                n_taxis = int(np.random.normal(89.8, 28.1))
        elif borough == 'Bronx':
            if day in [6]:
                n_requesters = int(np.random.normal(5.3, 2.8))
                n_taxis = int(np.random.normal(5.3, 2.7))
            else:
                n_requesters = int(np.random.normal(6.2, 2.6))
                n_taxis = int(np.random.normal(6.2, 2.7))
        elif borough == 'Brooklyn':
            if day in [6]:
                n_requesters = int(np.random.normal(26.5, 5.7))
                n_taxis = int(np.random.normal(26.4, 5.6))
            else:
                n_requesters = int(np.random.normal(27.2, 5.3))
                n_taxis = int(np.random.normal(26.7, 7.3))
        else:
            # Default fallback
            n_requesters = int(np.random.normal(50, 15))
            n_taxis = int(np.random.normal(45, 12))
        
        # Ensure positive numbers
        n_requesters = max(1, n_requesters)
        n_taxis = max(1, n_taxis)
        
        # Generate requester data with realistic distributions
        requesters_data = []
        for i in range(n_requesters):
            pu_location = np.random.randint(1, 264)
            do_location = np.random.randint(1, 264)
            
            # More realistic trip distance distribution
            trip_distance_km = np.random.lognormal(1.0, 0.8)  # Log-normal for realistic distances
            trip_distance_km = max(0.1, min(trip_distance_km, 25.0))
            
            # Generate total amount based on realistic NYC taxi pricing
            base_fare = 2.50
            rate_per_km = np.random.normal(2.75, 0.5)  # More realistic rate variation
            rate_per_km = max(1.5, rate_per_km)
            total_amount = base_fare + rate_per_km * trip_distance_km
            total_amount = max(1.0, total_amount)
            
            # Trip duration based on distance and NYC traffic
            base_duration = 300 + trip_distance_km * 120  # Base time + distance factor
            traffic_factor = np.random.normal(1.0, 0.3)  # Traffic variation
            trip_duration = int(base_duration * max(0.5, traffic_factor))
            trip_duration = max(180, min(trip_duration, 3600))  # 3min to 1hour
            
            requesters_data.append([
                borough,
                pu_location,
                do_location, 
                trip_distance_km,
                total_amount,
                trip_duration
            ])
        
        requesters_df = pd.DataFrame(requesters_data, columns=[
            'borough', 'PULocationID', 'DOLocationID', 
            'trip_distance_km', 'total_amount', 'trip_duration_seconds'
        ])
        
        # Sort by distance (required by MAPS)
        requesters_df = requesters_df.sort_values('trip_distance_km').reset_index(drop=True)
        
        # Generate taxi data (available taxis)
        taxis_data = []
        for j in range(n_taxis):
            location = np.random.randint(1, 264)
            taxis_data.append(location)
        
        taxis_df = pd.DataFrame({'DOLocationID': taxis_data})
        
        logger.info(f"ðŸš— Generated Hikima-style data: {len(requesters_df)} requesters, {len(taxis_df)} taxis")
        logger.info(f"ðŸ“Š Stats: req Î¼={len(requesters_df)}, taxi Î¼={len(taxis_df)} (following Hikima Table 1)")
        
        return requesters_df, taxis_df
    
    def process_taxi_data_hikima_single_type(self, df: pd.DataFrame, taxi_type: str,
                                            year: int, month: int, day: int, borough: str, 
                                            time_start: datetime, time_end: datetime) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Process SINGLE taxi type data following EXACT Hikima methodology - NO data combination."""
        
        # Load area information
        area_info = self.load_area_information()
        
        # Get column names based on taxi type
        if taxi_type == 'green':
            pickup_col = 'lpep_pickup_datetime'
            dropoff_col = 'lpep_dropoff_datetime'
        elif taxi_type == 'yellow':
            pickup_col = 'tpep_pickup_datetime'
            dropoff_col = 'tpep_dropoff_datetime'
        else:  # fhv
            pickup_col = 'pickup_datetime'
            dropoff_col = 'dropOff_datetime'
        
        # Parse datetime columns
        df[pickup_col] = pd.to_datetime(df[pickup_col])
        df[dropoff_col] = pd.to_datetime(df[dropoff_col])
        
        # Use dynamic time range based on the experiment parameters (no hardcoding)
        # Hikima filters to the full experiment day range, not individual scenarios
        day_start_time = datetime(year, month, day, 0, 0, 0)  # Start of day
        day_end_time = datetime(year, month, day, 23, 59, 59)  # End of day
        
        # Select required columns and merge with area information
        if taxi_type in ['green', 'yellow']:
            df_filtered = df[[pickup_col, dropoff_col, 'PULocationID', 'DOLocationID', 'trip_distance', 'total_amount']]
        else:  # fhv
            df_filtered = df[[pickup_col, dropoff_col, 'PUlocationID', 'DOlocationID']]
            df_filtered = df_filtered.rename(columns={'PUlocationID': 'PULocationID', 'DOlocationID': 'DOLocationID'})
            # Add default values for missing columns in FHV
            df_filtered['trip_distance'] = 2.0  # Default distance
            df_filtered['total_amount'] = 15.0  # Default amount
        
        # Merge with area information
        df_merged = pd.merge(df_filtered, area_info, how="inner", left_on="PULocationID", right_on="LocationID")
        
        # Apply Hikima filters
        tripdata = df_merged[
            (df_merged["trip_distance"] > 1e-3) &
            (df_merged["total_amount"] > 1e-3) &
            (df_merged["borough"] == borough) &
            (df_merged["PULocationID"] < 264) &
            (df_merged["DOLocationID"] < 264) &
            (df_merged[pickup_col] > day_start_time) &
            (df_merged[pickup_col] < day_end_time)
        ]
        
        logger.info(f"ðŸš— {taxi_type.upper()} after filtering: {len(tripdata)} trips")
        
        # Target time for this scenario - EXACT Hikima methodology
        # Hikima uses set_time as the start of the time window
        target_time = time_start
        
        # Use EXACT parameterized time window from scenario
        time_interval_seconds = (time_end - time_start).total_seconds()
        logger.info(f"â±ï¸ Using EXACT scenario time interval: {time_interval_seconds}s from {time_start} to {time_end}")
        
        # Extract requesters: pickup times within time interval  
        requesters = tripdata[
            (tripdata[pickup_col] > target_time) &
            (tripdata[pickup_col] < target_time + timedelta(seconds=time_interval_seconds))
        ]
        
        # Extract taxis: dropoff times within time interval (these become available taxis)
        taxis = tripdata[
            (tripdata[dropoff_col] > target_time) &
            (tripdata[dropoff_col] < target_time + timedelta(seconds=time_interval_seconds))
        ]
        
        if len(requesters) == 0 or len(taxis) == 0:
            logger.warning(f"âš ï¸ No requesters ({len(requesters)}) or taxis ({len(taxis)}) found for {target_time}")
            return pd.DataFrame(), pd.DataFrame()
        
        # Convert to numpy arrays for processing (following Hikima's exact structure)
        df_requesters = requesters.values
        df_taxis = taxis['DOLocationID'].values
        
        # Calculate trip duration in seconds (Hikima methodology)
        time_consume = np.zeros([df_requesters.shape[0], 1])
        for i in range(df_requesters.shape[0]):
            # Handle both datetime objects and timestamp floats (PyArrow compatibility)
            pickup_time = df_requesters[i, 5]
            dropoff_time = df_requesters[i, 6]
            
            if hasattr(pickup_time, 'total_seconds') and hasattr(dropoff_time, 'total_seconds'):
                # DateTime objects
                time_consume[i] = (dropoff_time - pickup_time).total_seconds()
            else:
                # Timestamp floats (seconds since epoch) - direct subtraction gives seconds
                time_consume[i] = float(dropoff_time) - float(pickup_time)
        
        # Combine requesters data: [borough, PULocationID, DOLocationID, trip_distance, total_amount, duration]
        df_requesters = np.hstack([df_requesters[:, 0:5], time_consume])
        
        # Delete data with too little distance traveled (Hikima filters)
        df_requesters = df_requesters[df_requesters[:, 3] > 1e-3]
        df_requesters = df_requesters[df_requesters[:, 4] > 1e-3]
        
        # Sort by distance in ascending order (required by MAPS)
        df_requesters = df_requesters[np.argsort(df_requesters[:, 3])]
        
        # Convert distance to km (Hikima: miles to km)
        df_requesters[:, 3] = df_requesters[:, 3] * 1.60934
        
        # Add Gaussian noise to coordinates (EXACT Hikima parameters)
        df_requesters_processed = self.add_hikima_coordinate_noise(df_requesters, df_taxis, area_info)
        df_taxis_processed = pd.DataFrame({'DOLocationID': df_taxis})
        
        logger.info(f"ðŸš— Hikima processed {taxi_type}: {len(df_requesters_processed)} requesters, {len(df_taxis_processed)} taxis")
        
        return df_requesters_processed, df_taxis_processed
    
    def add_hikima_coordinate_noise(self, df_requesters: np.ndarray, df_taxis: np.ndarray, area_info: pd.DataFrame) -> pd.DataFrame:
        """Add Gaussian noise to coordinates using EXACT Hikima parameters."""
        
        n = df_requesters.shape[0]
        m = df_taxis.shape[0]
        
        # Requester pickup coordinates with Gaussian noise
        requester_pickup_lat = []
        requester_pickup_lon = []
        requester_dropoff_lat = []
        requester_dropoff_lon = []
        
        for i in range(n):
            pu_location_id = self.safe_int_convert(df_requesters[i, 1]) - 1  # Convert to 0-based index
            do_location_id = self.safe_int_convert(df_requesters[i, 2]) - 1
            
            # Get base coordinates for pickup
            if pu_location_id < len(area_info):
                base_pu_lat = area_info.iloc[pu_location_id]['latitude']
                base_pu_lon = area_info.iloc[pu_location_id]['longitude']
            else:
                base_pu_lat = 40.7589
                base_pu_lon = -73.9851
            
            # Get base coordinates for dropoff
            if do_location_id < len(area_info):
                base_do_lat = area_info.iloc[do_location_id]['latitude'] 
                base_do_lon = area_info.iloc[do_location_id]['longitude']
            else:
                base_do_lat = 40.7589
                base_do_lon = -73.9851
            
            # Add EXACT Hikima Gaussian noise
            requester_pickup_lat.append(base_pu_lat + np.random.normal(0, 0.00306))
            requester_pickup_lon.append(base_pu_lon + np.random.normal(0, 0.000896))
            requester_dropoff_lat.append(base_do_lat + np.random.normal(0, 0.00306))
            requester_dropoff_lon.append(base_do_lon + np.random.normal(0, 0.000896))
        
        # Taxi coordinates with Gaussian noise
        taxi_lat = []
        taxi_lon = []
        
        for j in range(m):
            location_id = self.safe_int_convert(df_taxis[j]) - 1  # Convert to 0-based index
            
            if location_id < len(area_info):
                base_lat = area_info.iloc[location_id]['latitude']
                base_lon = area_info.iloc[location_id]['longitude'] 
            else:
                base_lat = 40.7589
                base_lon = -73.9851
            
            taxi_lat.append(base_lat + np.random.normal(0, 0.00306))
            taxi_lon.append(base_lon + np.random.normal(0, 0.000896))
        
        # Create final DataFrame following Hikima structure
        requesters_df = pd.DataFrame({
            'borough': [df_requesters[i, 0] for i in range(n)],
            'PULocationID': [self.safe_int_convert(df_requesters[i, 1]) for i in range(n)],
            'DOLocationID': [self.safe_int_convert(df_requesters[i, 2]) for i in range(n)],
            'trip_distance_km': [df_requesters[i, 3] for i in range(n)],
            'total_amount': [df_requesters[i, 4] for i in range(n)],
            'trip_duration_seconds': [df_requesters[i, 5] for i in range(n)],
            'pickup_lat': requester_pickup_lat,
            'pickup_lon': requester_pickup_lon,
            'dropoff_lat': requester_dropoff_lat,
            'dropoff_lon': requester_dropoff_lon,
            'taxi_coordinates': [(taxi_lat, taxi_lon)] * n  # Add taxi coordinates for edge weight calculation
        })
        
        return requesters_df
    
    def calculate_distance_matrix_and_edge_weights(self, requesters_df: pd.DataFrame, taxis_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Calculate distance matrix and edge weights following Hikima's exact methodology."""
        n = len(requesters_df)
        m = len(taxis_df)
        
        if n == 0 or m == 0:
            return np.zeros((n, m)), np.zeros((n, m))
        
        distance_matrix = np.zeros((n, m))
        edge_weights = np.zeros((n, m))
        
        logger.info(f"ðŸ”§ Calculating Hikima distance matrix and edge weights: {n}Ã—{m}")
        
        for i in range(n):
            # Get requester coordinates (with Gaussian noise already added)
            if 'pickup_lat' in requesters_df.columns:
                req_pickup_lat = requesters_df.iloc[i]['pickup_lat']
                req_pickup_lon = requesters_df.iloc[i]['pickup_lon']
                req_dropoff_lat = requesters_df.iloc[i]['dropoff_lat']
                req_dropoff_lon = requesters_df.iloc[i]['dropoff_lon']
            else:
                # Fallback if coordinates not available
                req_pickup_lat = 40.7589
                req_pickup_lon = -73.9851
                req_dropoff_lat = 40.7489
                req_dropoff_lon = -73.9951
            
            for j in range(m):
                # Get taxi coordinates (with Gaussian noise already added)
                if 'taxi_lat' in taxis_df.columns:
                    taxi_lat = taxis_df.iloc[j]['taxi_lat']
                    taxi_lon = taxis_df.iloc[j]['taxi_lon']
                else:
                    # Fallback if coordinates not available
                    taxi_lat = 40.7589
                    taxi_lon = -73.9851
                
                # Calculate distance from taxi to pickup location
                lat_diff = (req_pickup_lat - taxi_lat) * 111.0  # ~111 km per degree
                lon_diff = (req_pickup_lon - taxi_lon) * 111.0 * math.cos(math.radians(req_pickup_lat))
                taxi_to_pickup_km = math.sqrt(lat_diff**2 + lon_diff**2)
                
                # Get trip distance from data
                trip_distance_km = requesters_df.iloc[i]['trip_distance_km']
                
                # Total distance = taxi to pickup + trip distance
                total_distance_km = taxi_to_pickup_km + trip_distance_km
                distance_matrix[i, j] = total_distance_km
                
                # EXACT Hikima edge weight calculation:
                # W[i,j] = -(distance_ij[i,j] + df_requesters[i,3]) / s_taxi * alpha
                # where distance_ij[i,j] is taxi-to-pickup distance, df_requesters[i,3] is trip distance
                edge_weights[i, j] = -(taxi_to_pickup_km + trip_distance_km) / self.s_taxi * self.alpha
        
        logger.info(f"ðŸ“Š Distance matrix: mean={np.mean(distance_matrix):.2f}km, max={np.max(distance_matrix):.2f}km")
        logger.info(f"ðŸ“Š Edge weights: finite edges={np.sum(~np.isinf(edge_weights))}/{n*m}")
        
        return distance_matrix, edge_weights
    
    def calculate_acceptance_probability_hikima(self, prices: np.ndarray, trip_amounts: np.ndarray, acceptance_function: str) -> np.ndarray:
        """Calculate acceptance probabilities using EXACT Hikima formulas from their implementation."""
        if acceptance_function == 'PL':
            # EXACT Hikima PL formula from their code:
            # Acceptance_probability_proposed=-2.0/df_requesters[:,4]*price_proposed+3
            acceptance_probs = -2.0 / trip_amounts * prices + 3.0
            
        elif acceptance_function == 'Sigmoid':
            # EXACT Hikima Sigmoid formula from their code:
            # Acceptance_probability_proposed=1-(1/(1+np.exp(((-price_proposed+beta*df_requesters[:,4])/(gamma*df_requesters[:,4])).astype(np.float64))))
            beta = 1.3
            gamma = (0.3 * np.sqrt(3) / np.pi).astype(np.float64)
            
            exponent = ((-prices + beta * trip_amounts) / (gamma * trip_amounts)).astype(np.float64)
            exponent = np.clip(exponent, -50, 50)  # Prevent overflow
            acceptance_probs = 1 - (1 / (1 + np.exp(exponent)))
            
        else:
            raise ValueError(f"Unknown acceptance function: {acceptance_function}")
        
        return np.clip(acceptance_probs, 0.0, 1.0)
    
    def train_linucb_model(self, vehicle_type: str, borough: str, training_year: int, 
                          training_month: int, base_price: float = 5.875, 
                          price_multipliers: List[float] = None) -> Dict[str, Any]:
        """Train LinUCB model using historical data following the provided methodology."""
        
        if price_multipliers is None:
            price_multipliers = [0.6, 0.8, 1.0, 1.2, 1.4]
        
        start_time = time.time()
        logger.info(f"ðŸ”§ Training LinUCB model: {vehicle_type} taxis in {borough}, {training_year}-{training_month:02d}")
        
        # Initialize matrices for each arm
        num_arms = len(price_multipliers)
        A_matrices = {}
        b_vectors = {}
        
        # Load area information for creating features
        area_info = self.load_area_information()
        
        # Get area IDs for this borough
        df_borough = area_info[area_info["borough"] == borough]
        pu_id_set = list(set(df_borough['LocationID'].values))
        do_id_set = list(set(df_borough['LocationID'].values))
        
        # Feature dimension: 10 (hours) + len(pu_ids) + len(do_ids) + 2 (distance, duration)
        feature_dim = 10 + len(pu_id_set) + len(do_id_set) + 2
        
        for arm in range(num_arms):
            A_matrices[arm] = np.zeros((feature_dim, feature_dim))
            b_vectors[arm] = np.zeros(feature_dim)
        
        logger.info(f"ðŸ“Š Feature dimension: {feature_dim} (hours:10, PU:{len(pu_id_set)}, DO:{len(do_id_set)}, other:2)")
        
        # Process training data day by day
        total_samples = 0
        
        # Get number of days in the training month
        import calendar
        num_days = calendar.monthrange(training_year, training_month)[1]
        
        for training_day in range(1, num_days + 1):
            logger.info(f"ðŸ“… Training on day {training_day}/{num_days}")
            
            # Define day time range (10:00 to 20:00)
            day_start = datetime(training_year, training_month, training_day, 10, 0, 0)
            day_end = datetime(training_year, training_month, training_day, 20, 0, 0)
            
            try:
                # Load training data for this day
                requesters_df, taxis_df = self.load_taxi_data(
                    vehicle_type, training_year, training_month, training_day,
                    borough, day_start, day_end
                )
                
                if len(requesters_df) == 0:
                    logger.warning(f"No requester data for day {training_day}, skipping")
                    continue
                
                # Process 120 scenarios for this day (every 5 minutes from 10:00 to 20:00)
                for scenario_idx in range(120):
                    scenario_minute = scenario_idx * 5
                    current_hour = 10 + (scenario_minute // 60)
                    current_minute = scenario_minute % 60
                    
                    scenario_start = datetime(training_year, training_month, training_day, current_hour, current_minute, 0)
                    scenario_end = scenario_start + timedelta(minutes=5)
                    
                    # Get scenario data
                    scenario_requesters, scenario_taxis = self.load_taxi_data(
                        vehicle_type, training_year, training_month, training_day,
                        borough, scenario_start, scenario_end
                    )
                    
                    if len(scenario_requesters) == 0:
                        continue
                    
                    n = len(scenario_requesters)
                    m = len(scenario_taxis)
                    
                    # Calculate distance matrix and edge weights
                    distance_matrix, edge_weights = self.calculate_distance_matrix_and_edge_weights(
                        scenario_requesters, scenario_taxis
                    )
                    
                    # For each requester, randomly select an arm and calculate reward
                    trip_distances = scenario_requesters['trip_distance_km'].values
                    trip_amounts = scenario_requesters['total_amount'].values
                    
                    for i in range(n):
                        # Randomly select arm for training
                        random_arm = np.random.randint(0, num_arms)
                        
                        # Calculate price for this arm
                        price = base_price * price_multipliers[random_arm] * trip_distances[i]
                        
                        # Calculate acceptance probability using PL function (training uses PL)
                        acceptance_prob = self.calculate_acceptance_probability_hikima(
                            np.array([price]), np.array([trip_amounts[i]]), 'PL'
                        )[0]
                        
                        # Simulate acceptance
                        accepted = np.random.binomial(1, acceptance_prob)
                        
                        # Calculate reward through matching if accepted
                        reward = 0.0
                        if accepted and m > 0:
                            # Find best taxi for this requester
                            best_taxi = -1
                            best_reward = -np.inf
                            
                            for j in range(m):
                                if not np.isinf(edge_weights[i, j]):
                                    candidate_reward = price + edge_weights[i, j]
                                    if candidate_reward > best_reward:
                                        best_reward = candidate_reward
                                        best_taxi = j
                            
                            if best_taxi >= 0:
                                reward = best_reward
                        
                        # Create feature vector
                        hour_onehot = np.zeros(10)
                        hour_onehot[current_hour - 10] = 1
                        
                        pu_id = self.safe_int_convert(scenario_requesters.iloc[i]['PULocationID'])
                        do_id = self.safe_int_convert(scenario_requesters.iloc[i]['DOLocationID'])
                        
                        pu_onehot = np.zeros(len(pu_id_set))
                        if pu_id in pu_id_set:
                            pu_idx = pu_id_set.index(pu_id)
                            pu_onehot[pu_idx] = 1
                        
                        do_onehot = np.zeros(len(do_id_set))
                        if do_id in do_id_set:
                            do_idx = do_id_set.index(do_id)
                            do_onehot[do_idx] = 1
                        
                        # Combine features
                        features = np.concatenate([
                            hour_onehot,
                            pu_onehot,
                            do_onehot,
                            [trip_distances[i]],
                            [scenario_requesters.iloc[i]['trip_duration_seconds']]
                        ])
                        
                        # Update matrices for the selected arm
                        A_matrices[random_arm] += np.outer(features, features)
                        b_vectors[random_arm] += features * reward
                        
                        total_samples += 1
                        
                        if total_samples % 1000 == 0:
                            logger.info(f"   Processed {total_samples} samples")
                
            except Exception as e:
                logger.warning(f"âš ï¸ Error processing day {training_day}: {e}")
                continue
        
        # Save trained model to S3
        model_data = {
            'A_matrices': {str(k): v.tolist() for k, v in A_matrices.items()},
            'b_vectors': {str(k): v.tolist() for k, v in b_vectors.items()},
            'feature_dim': feature_dim,
            'pu_id_set': pu_id_set,
            'do_id_set': do_id_set,
            'base_price': base_price,
            'price_multipliers': price_multipliers,
            'total_samples': total_samples,
            'training_period': f"{training_year}-{training_month:02d}",
            'borough': borough,
            'vehicle_type': vehicle_type
        }
        
        # Save to S3 using original Hikima format (separate pickle files for each arm)
        try:
            saved_keys = []
            month_suffix = f"{training_month:02d}"
            month_code = f"{training_year}{month_suffix}"
            
            # Save A matrices and b vectors for each arm separately (following original format)
            for arm in range(5):
                # Save A matrix
                A_key = f"models/linucb/{vehicle_type}_{borough}_{month_code}/A_{arm}_{month_suffix}"
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=A_key,
                    Body=pickle.dumps(A_matrices[arm]),
                    ContentType='application/octet-stream'
                )
                saved_keys.append(A_key)
                
                # Save b vector
                b_key = f"models/linucb/{vehicle_type}_{borough}_{month_code}/b_{arm}_{month_suffix}"
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=b_key,
                    Body=pickle.dumps(b_vectors[arm]),
                    ContentType='application/octet-stream'
                )
                saved_keys.append(b_key)
                
                logger.info(f"ðŸ’¾ Saved LinUCB arm {arm} to S3: A_{arm}_{month_suffix}, b_{arm}_{month_suffix}")
            
            training_time = time.time() - start_time
            logger.info(f"âœ… LinUCB training completed: {total_samples} samples, {training_time:.1f}s")
            logger.info(f"ðŸ’¾ Model saved in Hikima format: {len(saved_keys)} files")
            
            return {
                'success': True,
                'saved_keys': saved_keys,
                'total_samples': total_samples,
                'training_time': training_time,
                'feature_dimension': feature_dim,
                'format': 'hikima_original'
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to save LinUCB model: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def evaluate_matching_hikima(self, prices: np.ndarray, acceptance_results: np.ndarray, 
                                edge_weights: np.ndarray) -> Tuple[float, List[Tuple[int, int]]]:
        """Evaluate objective value using Hikima's bipartite matching approach with precomputed edge weights."""
        n, m = edge_weights.shape
        
        # Create bipartite graph
        G = nx.Graph()
        group1 = range(n)
        group2 = range(n, n + m)
        G.add_nodes_from(group1, bipartite=1)
        G.add_nodes_from(group2, bipartite=0)
        
        # Add edges only for accepted requests and finite edge weights
        for i in range(n):
            if acceptance_results[i] == 1:
                for j in range(m):
                    if not np.isinf(edge_weights[i, j]):  # Only add edges with finite weights
                        weight = prices[i] + edge_weights[i, j]
                    G.add_edge(i, n + j, weight=weight)
        
        # Find maximum weight matching
        try:
            matched_edges = nx.max_weight_matching(G)
            
            # Calculate objective value
            objective_value = 0.0
            matches = []
            
            for edge in matched_edges:
                i, j_plus_n = edge
                if i > j_plus_n:
                    i, j_plus_n = j_plus_n, i
                j = j_plus_n - n
                
                if 0 <= i < n and 0 <= j < m:
                    reward = prices[i] + edge_weights[i, j]
                    objective_value += reward
                    matches.append((i, j))
            
            return objective_value, matches
        except Exception as e:
            logger.warning(f"âš ï¸ Matching failed: {e}")
            return 0.0, []
    
    def run_minmaxcostflow(self, requesters_df: pd.DataFrame, taxis_df: pd.DataFrame, 
                          distance_matrix: np.ndarray, edge_weights: np.ndarray, acceptance_function: str) -> Dict[str, Any]:
        """MinMaxCostFlow implementation based on Hikima's min-cost flow algorithm."""
        start_time = time.time()
        
        n = len(requesters_df)
        m = len(taxis_df)
        
        if n == 0 or m == 0:
            return {
                'method_name': 'MinMaxCostFlow',
                'objective_value': 0.0,
                'computation_time': time.time() - start_time,
                'num_requests': n,
                'num_taxis': m
            }
        
        logger.info(f"ðŸ”§ Running MinMaxCostFlow: {n} requests, {m} taxis")
        
        try:
            # Initialize flow variables (simplified version)
            flow_variables = np.zeros(n)  # Flow from source to each requester
            
            # Iterative algorithm to find optimal flows
            delta = 1.0
            trip_amounts = requesters_df['total_amount'].values
        
            # Simplified capacity scaling approach following Hikima
            while delta > 0.001:
                for i in range(n):
                    # Calculate cost function derivatives for acceptance probability
                    if acceptance_function == 'Sigmoid':
                        # For sigmoid: derive optimal price from flow
                        flow_val = max(0.001, min(0.999, flow_variables[i]))
                        beta = 1.3
                        gamma = 0.3 * np.sqrt(3) / np.pi
                        # Inverse sigmoid to get price from probability
                        if flow_val < 0.999:
                            price = beta * trip_amounts[i] - gamma * np.abs(trip_amounts[i]) * np.log((1 - flow_val) / flow_val)
                        else:
                            price = beta * trip_amounts[i]
                    else:  # PL
                        # For piecewise linear: inverse function
                        flow_val = max(0.001, min(0.999, flow_variables[i]))
                        alpha = 1.5
                        qu = trip_amounts[i]
                        if flow_val == 1.0:
                            price = qu * 0.9  # Just below qu
                        else:
                            # From p = -1/((Î±-1)*qu) * x + Î±/(Î±-1), solve for x
                            price = ((alpha / (alpha - 1)) - flow_val) * (alpha - 1) * qu
                            price = np.clip(price, qu, alpha * qu)
                    
                    # Update flow based on cost gradients (simplified)
                    cost_gradient = self.calculate_cost_gradient(flow_val, trip_amounts[i], acceptance_function)
                    flow_variables[i] = max(0.0, min(1.0, flow_variables[i] - delta * cost_gradient))
                
                delta *= 0.5
            
            # Calculate final prices from flows
            prices = np.zeros(n)
            for i in range(n):
                flow_val = max(0.001, min(0.999, flow_variables[i]))
                if acceptance_function == 'Sigmoid':
                    beta = 1.3
                    gamma = 0.3 * np.sqrt(3) / np.pi
                    if flow_val < 0.999:
                        prices[i] = beta * trip_amounts[i] - gamma * np.abs(trip_amounts[i]) * np.log((1 - flow_val) / flow_val)
                    else:
                        prices[i] = beta * trip_amounts[i]
                else:  # PL
                    alpha = 1.5
                    qu = trip_amounts[i]
                    if flow_val == 1.0:
                        prices[i] = qu * 0.9
                    else:
                        prices[i] = ((alpha / (alpha - 1)) - flow_val) * (alpha - 1) * qu
                        prices[i] = np.clip(prices[i], qu, alpha * qu)
            
            # Calculate acceptance probabilities using Hikima formulas
            acceptance_probs = self.calculate_acceptance_probability_hikima(prices, trip_amounts, acceptance_function)
            
            # Monte Carlo evaluation
            total_objective = 0.0
            for eval_iter in range(self.num_eval):
                acceptance_results = np.random.binomial(1, acceptance_probs)
                objective_value, matches = self.evaluate_matching_hikima(
                    prices, acceptance_results, edge_weights
                )
                total_objective += objective_value
            
            avg_objective = total_objective / self.num_eval
            
        except Exception as e:
            logger.error(f"âŒ MinMaxCostFlow error: {e}")
            avg_objective = 0.0
        
        computation_time = time.time() - start_time
        
        logger.info(f"âœ… MinMaxCostFlow completed: Objective={avg_objective:.2f}, Time={computation_time:.3f}s")
        
        return {
            'method_name': 'MinMaxCostFlow',
            'objective_value': avg_objective,
            'computation_time': computation_time,
            'num_requests': n,
            'num_taxis': m,
            'avg_acceptance_rate': float(np.mean(acceptance_probs)) if 'acceptance_probs' in locals() else 0.0
        }
    
    def calculate_cost_gradient(self, flow_val: float, trip_amount: float, acceptance_function: str) -> float:
        """Calculate cost function gradient for flow optimization."""
        if acceptance_function == 'Sigmoid':
            # Simplified gradient for sigmoid acceptance function
            return (flow_val - 0.5) * 0.1
        else:  # PL
            # Simplified gradient for piecewise linear
            return (flow_val - 0.5) * trip_amount * 0.01
    
    def run_maps(self, requesters_df: pd.DataFrame, taxis_df: pd.DataFrame, 
                distance_matrix: np.ndarray, edge_weights: np.ndarray, acceptance_function: str) -> Dict[str, Any]:
        """MAPS implementation following Hikima methodology."""
        start_time = time.time()
        
        n = len(requesters_df)
        if n == 0:
            return {
                'method_name': 'MAPS',
                'objective_value': 0.0,
                'computation_time': time.time() - start_time,
                'num_requests': 0,
                'num_taxis': len(taxis_df)
            }
        
        logger.info(f"ðŸ”§ Running MAPS: {n} requests, {len(taxis_df)} taxis")
        
        # Area-based pricing following MAPS methodology
        area_acceptance_probs = {}
        trip_amounts = requesters_df['total_amount'].values
        trip_distances = requesters_df['trip_distance_km'].values
        
        # Group by pickup location (area) and calculate average acceptance probability
        area_requesters = {}
        for i in range(n):
            pu_location = self.safe_int_convert(requesters_df.iloc[i]['PULocationID'])
            if pu_location not in area_requesters:
                area_requesters[pu_location] = []
            area_requesters[pu_location].append(i)
        
        # Calculate area-based acceptance probabilities by taking average within each area
        for area, requester_indices in area_requesters.items():
            area_trip_amounts = [trip_amounts[i] for i in requester_indices]
            avg_trip_amount = np.mean(area_trip_amounts)
            
            # Calculate price based on trip characteristics
            area_price = avg_trip_amount * 1.1  # 110% of average trip amount
            
            # Calculate average acceptance probability for this area
            area_acceptance_prob = self.calculate_acceptance_probability_hikima(
                np.array([area_price]), np.array([avg_trip_amount]), acceptance_function
            )[0]
            area_acceptance_probs[area] = area_acceptance_prob
        
        # Calculate prices for each request based on area pricing
        prices = np.zeros(n)
        for i in range(n):
            pu_location = self.safe_int_convert(requesters_df.iloc[i]['PULocationID'])
            # Price based on area and individual trip distance
            base_area_price = trip_amounts[i] * 1.1
            prices[i] = base_area_price
        
        # Calculate acceptance probabilities using Hikima formulas
        acceptance_probs = self.calculate_acceptance_probability_hikima(prices, trip_amounts, acceptance_function)
        
        # Run Monte Carlo evaluation
        total_objective = 0.0
        for eval_iter in range(self.num_eval):
            acceptance_results = np.random.binomial(1, acceptance_probs)
            objective_value, matches = self.evaluate_matching_hikima(
                prices, acceptance_results, edge_weights
            )
            total_objective += objective_value
        
        avg_objective = total_objective / self.num_eval
        computation_time = time.time() - start_time
        
        logger.info(f"âœ… MAPS completed: Objective={avg_objective:.2f}, Time={computation_time:.3f}s")
        
        return {
            'method_name': 'MAPS',
            'objective_value': avg_objective,
            'computation_time': computation_time,
            'num_requests': n,
            'num_taxis': len(taxis_df),
            'num_areas': len(area_acceptance_probs),
            'avg_acceptance_rate': float(np.mean(acceptance_probs))
        }
    
    def run_linucb(self, requesters_df: pd.DataFrame, taxis_df: pd.DataFrame, 
                  distance_matrix: np.ndarray, edge_weights: np.ndarray, acceptance_function: str,
                  borough: str, vehicle_type: str, current_hour: int) -> Dict[str, Any]:
        """LinUCB implementation using trained model."""
        start_time = time.time()
        
        n = len(requesters_df)
        if n == 0:
            return {
                'method_name': 'LinUCB',
                'objective_value': 0.0,
                'computation_time': time.time() - start_time,
                'num_requests': 0,
                'num_taxis': len(taxis_df),
                'avg_acceptance_rate': 0.0
            }
        
        logger.info(f"ðŸ”§ Running LinUCB: {n} requests, {len(taxis_df)} taxis")
        
        # Load trained LinUCB model using original Hikima methodology
        # Follow the exact approach from original code: load A_0-A_4 and b_0-b_4 from multiple months
        try:
            # Set up parameters exactly as in original Hikima code
            base_price = 5.875
            price_multipliers = np.array([0.6, 0.8, 1.0, 1.2, 1.4])
            
            # Available LinUCB models (hardcoded based on what we have)
            available_models = {
                ('yellow', 'Manhattan'): ['201907', '201908', '201909'],
                ('yellow', 'Brooklyn'): ['201907', '201908', '201909'],
                ('yellow', 'Queens'): ['201907', '201908', '201909'],
                ('yellow', 'Bronx'): ['201907', '201908', '201909'],
                ('green', 'Manhattan'): ['201907', '201908', '201909'],
                ('green', 'Brooklyn'): ['201907', '201908', '201909'],
                ('green', 'Queens'): ['201907', '201908', '201909'],
                ('green', 'Bronx'): ['201907', '201908', '201909']
            }
            
            # Check if model is available for this vehicle type and borough
            model_key = (vehicle_type, borough)
            if model_key not in available_models:
                logger.warning(f"âš ï¸ LinUCB model not available for {vehicle_type} in {borough}")
                raise Exception(f"No LinUCB model available for {vehicle_type} in {borough}")
            
            available_periods = available_models[model_key]
            logger.info(f"ðŸ“‹ Available LinUCB periods for {vehicle_type} {borough}: {available_periods}")
            
            # Get area info for PUID_set and DOID_set
            area_info = self.load_area_information()
            df_id = area_info[area_info["borough"] == borough]
            pu_id_set = list(set(df_id['LocationID'].values))
            do_id_set = list(set(df_id['LocationID'].values))
            
            # First, load one matrix to determine the actual feature dimension from pre-trained data
            feature_dim = None
            sample_A_key = f"models/linucb/{vehicle_type}_{borough}_201907/A_0_07"
            
            try:
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=sample_A_key)
                sample_A = pickle.loads(response['Body'].read())
                feature_dim = sample_A.shape[0]  # Get actual feature dimension from pre-trained data
                logger.info(f"ðŸ“ Detected feature dimension from pre-trained data: {feature_dim}")
            except Exception as e:
                logger.warning(f"âš ï¸ Could not load sample matrix to detect dimension: {e}")
                # Fall back to calculated dimension
                feature_dim = 10 + len(pu_id_set) + len(do_id_set) + 2
                logger.info(f"ðŸ“ Using calculated feature dimension: {feature_dim}")
            
            A_matrices = {}
            b_vectors = {}
            
            # Load and combine matrices from available periods exactly as in original
            for arm in range(5):
                A_arm = np.zeros((feature_dim, feature_dim))
                b_arm = np.zeros(feature_dim)
                
                # Load from available periods and combine as in original code
                for period in available_periods:
                    month_suffix = period[-2:]  # Get last 2 digits (07, 08, 09)
                    month_code = period
                    try:
                        # Load A matrix for this arm and month
                        A_key = f"models/linucb/{vehicle_type}_{borough}_{month_code}/A_{arm}_{month_suffix}"
                        response = self.s3_client.get_object(Bucket=self.bucket_name, Key=A_key)
                        A_month = pickle.loads(response['Body'].read())
                        
                        # Verify dimension compatibility
                        if A_month.shape[0] != feature_dim:
                            logger.warning(f"âš ï¸ Dimension mismatch for {A_key}: expected {feature_dim}x{feature_dim}, got {A_month.shape}")
                            continue
                        
                        A_arm += A_month
                        
                        # Load b vector for this arm and month
                        b_key = f"models/linucb/{vehicle_type}_{borough}_{month_code}/b_{arm}_{month_suffix}"
                        response = self.s3_client.get_object(Bucket=self.bucket_name, Key=b_key)
                        b_month = pickle.loads(response['Body'].read())
                        
                        # Verify dimension compatibility
                        if b_month.shape[0] != feature_dim:
                            logger.warning(f"âš ï¸ Dimension mismatch for {b_key}: expected {feature_dim}, got {b_month.shape}")
                            continue
                        
                        b_arm += b_month
                        
                        logger.info(f"âœ… Loaded LinUCB {month_code} data for arm {arm} (dim: {A_month.shape})")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ Could not load {month_code} data for arm {arm}: {e}")
                
                # Add identity matrix as in original code: A_arm += np.eye(A_arm.shape[0])
                A_arm += np.eye(feature_dim)
                
                A_matrices[arm] = A_arm
                b_vectors[arm] = b_arm
            
            logger.info(f"âœ… Loaded trained LinUCB model with {len(A_matrices)} arms, feature_dim={feature_dim}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load trained LinUCB model: {e}")
            # Use default LinUCB with small initialization as in original
            area_info = self.load_area_information()
            df_id = area_info[area_info["borough"] == borough]
            pu_id_set = list(set(df_id['LocationID'].values))
            do_id_set = list(set(df_id['LocationID'].values))
            
            base_price = 5.875
            price_multipliers = np.array([0.6, 0.8, 1.0, 1.2, 1.4])
            feature_dim = 10 + len(pu_id_set) + len(do_id_set) + 2
            
            A_matrices = {i: np.eye(feature_dim) for i in range(5)}
            b_vectors = {i: np.zeros(feature_dim) for i in range(5)}
        
        # Apply LinUCB with trained model
        trip_amounts = requesters_df['total_amount'].values
        trip_distances = requesters_df['trip_distance_km'].values
        prices = np.zeros(n)
        ucb_alpha = 0.5
        
        for i in range(n):
            # Create feature vector for this request
            hour_onehot = np.zeros(10)
            if 0 <= current_hour - 10 < 10:
                hour_onehot[current_hour - 10] = 1
            
            pu_id = self.safe_int_convert(requesters_df.iloc[i]['PULocationID'])
            do_id = self.safe_int_convert(requesters_df.iloc[i]['DOLocationID'])
            
            pu_onehot = np.zeros(len(pu_id_set))
            if pu_id in pu_id_set:
                pu_idx = pu_id_set.index(pu_id)
                pu_onehot[pu_idx] = 1
            
            do_onehot = np.zeros(len(do_id_set))
            if do_id in do_id_set:
                do_idx = do_id_set.index(do_id)
                do_onehot[do_idx] = 1
            
            # Combine features
            basic_features = np.concatenate([
                hour_onehot,
                pu_onehot,
                do_onehot,
                [trip_distances[i]],
                [requesters_df.iloc[i]['trip_duration_seconds']]
            ])
            
            # Pad or truncate to match pre-trained feature dimension
            if len(basic_features) < feature_dim:
                # Pad with zeros to match expected dimension
                features = np.zeros(feature_dim)
                features[:len(basic_features)] = basic_features
                logger.debug(f"Padded features from {len(basic_features)} to {feature_dim}")
            elif len(basic_features) > feature_dim:
                # Truncate to match expected dimension
                features = basic_features[:feature_dim]
                logger.debug(f"Truncated features from {len(basic_features)} to {feature_dim}")
            else:
                features = basic_features
            
            # Calculate UCB values for each arm
            best_arm = 0
            best_ucb = -np.inf
            
            for arm_idx in range(len(price_multipliers)):
                if arm_idx in A_matrices:
                    A = A_matrices[arm_idx]
                    b = b_vectors[arm_idx]
                    
                    # Add regularization
                    A_reg = A + np.eye(A.shape[0]) * 1e-6
                    
                    try:
                        # Calculate theta (coefficient vector)
                        theta = np.linalg.solve(A_reg, b)
                        
                        # Calculate confidence bound
                        confidence = ucb_alpha * np.sqrt(features.T @ np.linalg.solve(A_reg, features))
                        
                        # UCB value
                        ucb_value = features.T @ theta + confidence
                        
                        if ucb_value > best_ucb:
                            best_ucb = ucb_value
                            best_arm = arm_idx
                            
                    except np.linalg.LinAlgError:
                        # Fallback if matrix inversion fails
                        continue
            
            # Calculate price using selected arm
            chosen_multiplier = price_multipliers[best_arm]
            prices[i] = base_price * chosen_multiplier * trip_distances[i]
        
        # Calculate acceptance probabilities using specified function
        acceptance_probs = self.calculate_acceptance_probability_hikima(prices, trip_amounts, acceptance_function)
        
        # Run Monte Carlo evaluation
        total_objective = 0.0
        for eval_iter in range(self.num_eval):
            acceptance_results = np.random.binomial(1, acceptance_probs)
            objective_value, matches = self.evaluate_matching_hikima(
                prices, acceptance_results, edge_weights
            )
            total_objective += objective_value
        
        avg_objective = total_objective / self.num_eval
        computation_time = time.time() - start_time
        
        logger.info(f"âœ… LinUCB completed: Objective={avg_objective:.2f}, Time={computation_time:.3f}s")
        
        # Ensure acceptance_probs is valid
        avg_acceptance_rate = float(np.mean(acceptance_probs)) if len(acceptance_probs) > 0 and not np.all(np.isnan(acceptance_probs)) else 0.0
        
        return {
            'method_name': 'LinUCB',
            'objective_value': avg_objective,
            'computation_time': computation_time,
            'num_requests': n,
            'num_taxis': len(taxis_df),
            'num_arms': len(price_multipliers),
            'avg_acceptance_rate': avg_acceptance_rate,
            'model_format': 'hikima_original'
        }
    
    def run_lp(self, requesters_df: pd.DataFrame, taxis_df: pd.DataFrame, 
              distance_matrix: np.ndarray, edge_weights: np.ndarray, acceptance_function: str) -> Dict[str, Any]:
        """Gupta-Nagarajan Linear Program implementation."""
        start_time = time.time()
        
        n = len(requesters_df)
        m = len(taxis_df)
        
        if n == 0 or m == 0:
            return {
                'method_name': 'LP',
                'objective_value': 0.0,
                'computation_time': time.time() - start_time,
                'num_requests': n,
                'num_taxis': m,
                'avg_acceptance_rate': 0.0
            }
        
        logger.info(f"ðŸ”§ Running LP: {n} requests, {m} taxis")
        
        try:
            # Create simplified price grid following Hikima constraints
            trip_amounts = requesters_df['total_amount'].values
            
            price_grids = {}
            accept_probs = {}
            
            for i in range(n):
                # Create price grid for each customer following Hikima constraints
                qu = trip_amounts[i]  # Actually paid amount
                if acceptance_function == 'PL':
                    alpha = 1.5
                    # Price range [qu, Î±*qu] for PL function
                    prices = np.linspace(qu, alpha * qu, 5)
                else:  # Sigmoid
                    # Wider range for sigmoid
                    prices = np.linspace(qu * 0.8, qu * 2.0, 5)
                    
                price_grids[i] = prices
                
                # Calculate acceptance probabilities for each price using Hikima formulas
                for price in prices:
                    accept_prob = self.calculate_acceptance_probability_hikima(
                        np.array([price]), np.array([qu]), acceptance_function
                    )[0]
                    accept_probs[(i, price)] = accept_prob
            
            # Solve simplified LP
            prob = pl.LpProblem("RideHailing_LP", pl.LpMaximize)
            
            # Decision variables
            y_vars = {}
            x_vars = {}
            
            for i in range(n):
                for price in price_grids[i]:
                    y_vars[(i, price)] = pl.LpVariable(f"y_{i}_{price}", 0, 1, pl.LpContinuous)
                    
                    for j in range(m):
                        if not np.isinf(edge_weights[i, j]):  # Only create variables for finite edge weights
                            x_vars[(i, j, price)] = pl.LpVariable(f"x_{i}_{j}_{price}", 0, 1, pl.LpContinuous)
            
            # Objective: maximize expected profit
            objective_terms = []
            for i in range(n):
                for price in price_grids[i]:
                    for j in range(m):
                        if not np.isinf(edge_weights[i, j]):
                            profit = price + edge_weights[i, j]
                        objective_terms.append(profit * x_vars[(i, j, price)])
            
            prob += pl.lpSum(objective_terms)
            
            # Constraints
            # 1. At most one price per customer
            for i in range(n):
                prob += pl.lpSum(y_vars[(i, price)] for price in price_grids[i]) <= 1
            
            # 2. Acceptance constraints
            for i in range(n):
                for price in price_grids[i]:
                    lhs = pl.lpSum(x_vars[(i, j, price)] for j in range(m) if not np.isinf(edge_weights[i, j]))
                    rhs = accept_probs[(i, price)] * y_vars[(i, price)]
                    prob += lhs <= rhs
            
            # 3. Taxi capacity constraints
            for j in range(m):
                prob += pl.lpSum(x_vars[(i, j, price)] for i in range(n) for price in price_grids[i] 
                               if not np.isinf(edge_weights[i, j])) <= 1
            
            # Solve
            prob.solve(pl.PULP_CBC_CMD(msg=0, timeLimit=30))
            
            if prob.status == pl.LpStatusOptimal:
                objective_value = float(pl.value(prob.objective))
                logger.info(f"âœ… LP optimal solution found: {objective_value:.2f}")
            else:
                logger.warning(f"âš ï¸ LP solver status: {pl.LpStatus[prob.status]}")
                objective_value = 0.0
                
            # Calculate average acceptance rate from all computed probabilities
            all_accept_probs = list(accept_probs.values())
            avg_acceptance_rate = float(np.mean(all_accept_probs)) if all_accept_probs else 0.0
            
        except Exception as e:
            logger.error(f"âŒ LP solver error: {e}")
            objective_value = 0.0
            avg_acceptance_rate = 0.0
        
        computation_time = time.time() - start_time
        
        logger.info(f"âœ… LP completed: Objective={objective_value:.2f}, Time={computation_time:.3f}s")
        
        return {
            'method_name': 'LP',
            'objective_value': objective_value,
            'computation_time': computation_time,
            'num_requests': n,
            'num_taxis': m,
            'avg_acceptance_rate': avg_acceptance_rate,
            'solver_status': pl.LpStatus.get(prob.status, 'Unknown') if 'prob' in locals() else 'Error'
        }
    
    def save_results_to_s3(self, results: List[Dict[str, Any]], event: Dict[str, Any], 
                          data_stats: Dict[str, Any], performance_summary: Dict[str, Any]) -> str:
        """Save experiment results to S3 with new directory structure."""
        try:
            import random
            import pandas as pd
            
            # Extract event parameters
            vehicle_type = event.get('vehicle_type', 'green')
            acceptance_function = event.get('acceptance_function', 'PL')
            year = event.get('year', 2019)
            month = event.get('month', 10)
            day = event.get('day', 1)
            borough = event.get('borough', 'Manhattan')
            scenario_index = event.get('scenario_index', 0)
            time_window = event.get('time_window', {})
            training_id = event.get('training_id', 'unknown')
            
            # Generate random 5-digit experiment ID
            experiment_id = f"{random.randint(10000, 99999)}"
            
            # Create timestamp for execution
            execution_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # Create directory name: {experiment_id}_{timestamp}
            directory_name = f"{experiment_id}_{execution_timestamp}"
            
            # Build base S3 path
            base_path = f"experiments/type={vehicle_type}/eval={acceptance_function}/borough={borough}/year={year}/month={month:02d}/day={day:02d}/{directory_name}"
            
            # Calculate time window details
            hour_start = time_window.get('hour_start', 10)
            minute_start = time_window.get('minute_start', 0)
            time_interval = time_window.get('time_interval', 5)
            
            total_minutes = scenario_index * time_interval
            current_hour = hour_start + (total_minutes // 60)
            current_minute = minute_start + (total_minutes % 60)
            
            if current_minute >= 60:
                current_hour += current_minute // 60
                current_minute = current_minute % 60
            
            time_start = f"{current_hour:02d}:{current_minute:02d}"
            end_minute = current_minute + time_interval
            end_hour = current_hour
            if end_minute >= 60:
                end_hour += 1
                end_minute -= 60
            time_end = f"{end_hour:02d}:{end_minute:02d}"
            
            # Calculate matching success percentage
            matching_stats = self.calculate_matching_success_stats(results, data_stats)
            
            # 1. Create experiment_summary.json
            experiment_summary = {
                'experiment_metadata': {
                    'experiment_id': experiment_id,
                    'execution_timestamp': execution_timestamp,
                    'seed': random.getstate()[1][0] if hasattr(random.getstate()[1], '__getitem__') else None,
                    'vehicle_type': vehicle_type,
                    'acceptance_function': acceptance_function,
                    'borough': borough,
                    'year': year,
                    'month': month,
                    'day': day,
                    'scenario_index': scenario_index,
                    'training_id': training_id
                },
                'time_window': {
                    'start': time_start,
                    'end': time_end,
                    'duration_minutes': time_interval,
                    'hour_start': hour_start,
                    'hour_end': time_window.get('hour_end', 20),
                    'time_interval': time_interval
                },
                'experiment_setup': {
                    'methods': event.get('methods', []),
                    'execution_date': event.get('execution_date'),
                    'parallel_workers': event.get('parallel', 3),
                    'production_mode': event.get('production', False),
                    'debug_mode': event.get('debug', False),
                    'num_eval': event.get('num_eval', 1000)  # Number of Monte Carlo simulations
                },
                'data_statistics': data_stats,
                'matching_statistics': matching_stats,
                'performance_summary': performance_summary,
                'results_basic_analysis': {
                    'total_methods': len(results),
                    'successful_methods': sum(1 for r in results if 'error' not in r),
                    'failed_methods': sum(1 for r in results if 'error' in r),
                    'total_objective_value': sum(r.get('objective_value', 0) for r in results),
                    'total_computation_time': sum(r.get('computation_time', 0) for r in results),
                    'avg_computation_time': sum(r.get('computation_time', 0) for r in results) / len(results) if results else 0,
                    'best_method': max(results, key=lambda x: x.get('objective_value', 0))['method_name'] if results else None,
                    'best_objective_value': max(r.get('objective_value', 0) for r in results) if results else 0
                }
            }
            
            # Upload experiment_summary.json
            summary_key = f"{base_path}/experiment_summary.json"
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=summary_key,
                Body=json.dumps(experiment_summary, indent=2, default=str),
                ContentType='application/json'
            )
            
            # 2. Create results.parquet with detailed scenario data
            results_data = []
            
            for result in results:
                result_row = {
                    'experiment_id': experiment_id,
                    'execution_timestamp': execution_timestamp,
                    'scenario_index': scenario_index,
                    'method_name': result.get('method_name', 'Unknown'),
                    'objective_value': result.get('objective_value', 0),
                    'computation_time': result.get('computation_time', 0),
                    'success': 'error' not in result,
                    'error_message': result.get('error', None),
                    'num_requests': result.get('num_requests', 0),
                    'num_taxis': result.get('num_taxis', 0),
                    'avg_acceptance_rate': result.get('avg_acceptance_rate', 0),
                    'time_start': time_start,
                    'time_end': time_end,
                    'duration_minutes': time_interval,
                    'vehicle_type': vehicle_type,
                    'acceptance_function': acceptance_function,
                    'borough': borough,
                    'year': year,
                    'month': month,
                    'day': day
                }
                
                # Add method-specific metrics
                for key, value in result.items():
                    if key not in ['method_name', 'objective_value', 'computation_time', 'error', 'num_requests', 'num_taxis', 'avg_acceptance_rate']:
                        result_row[f'method_{key}'] = value
                
                results_data.append(result_row)
            
            # Convert to DataFrame and save as parquet
            if results_data:
                results_df = pd.DataFrame(results_data)
                
                # Convert DataFrame to parquet bytes
                parquet_buffer = io.BytesIO()
                results_df.to_parquet(parquet_buffer, index=False, engine='pyarrow')
                parquet_bytes = parquet_buffer.getvalue()
                
                # Upload results.parquet
                results_key = f"{base_path}/results.parquet"
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=results_key,
                    Body=parquet_bytes,
                    ContentType='application/octet-stream'
                )
            
            s3_location = f"s3://{self.bucket_name}/{base_path}/"
            logger.info(f"âœ… Experiment saved: {s3_location}")
            logger.info(f"ðŸ“Š Files: experiment_summary.json, results.parquet")
            logger.info(f"ðŸ”¢ Experiment ID: {experiment_id}")
            
            return s3_location
            
        except Exception as e:
            logger.error(f"âŒ Failed to save results to S3: {e}")
            return ""
    
    def calculate_matching_success_stats(self, results: List[Dict[str, Any]], data_stats: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate matching success statistics."""
        try:
            num_requesters = data_stats.get('num_requesters', 0)
            num_taxis = data_stats.get('num_taxis', 0)
            
            # Calculate theoretical maximum matches
            max_possible_matches = min(num_requesters, num_taxis)
            
            # Calculate actual matching statistics from results
            matching_stats = {
                'max_possible_matches': max_possible_matches,
                'theoretical_matching_rate': min(1.0, num_taxis / max(1, num_requesters)),
                'methods': {}
            }
            
            for result in results:
                method_name = result.get('method_name', 'Unknown')
                
                # Try to extract matching information from the result
                matches_count = 0
                acceptance_rate = result.get('avg_acceptance_rate', 0)
                
                # Estimate successful matches based on acceptance rate and available taxis
                if acceptance_rate > 0 and num_requesters > 0:
                    accepted_requests = int(num_requesters * acceptance_rate)
                    matches_count = min(accepted_requests, num_taxis)
                
                matching_success_rate = matches_count / max(1, num_requesters)
                
                matching_stats['methods'][method_name] = {
                    'estimated_matches': matches_count,
                    'matching_success_percentage': matching_success_rate * 100,
                    'acceptance_rate': acceptance_rate,
                    'efficiency_ratio': matches_count / max(1, max_possible_matches)
                }
            
            return matching_stats
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error calculating matching stats: {e}")
            return {'error': str(e)}


def lambda_handler(event, context):
    """
    AWS Lambda handler function for ride-hailing pricing experiments.
    Supports both single scenario and batch processing for improved performance.
    """
    logger.info(f"ðŸ“¥ Lambda invoked: {json.dumps(event, default=str)}")
    
    # Check remaining time
    remaining_time = context.get_remaining_time_in_millis() if context else 900000
    logger.info(f"â±ï¸ Initial remaining time: {remaining_time/1000:.1f}s")
    
    # Handle test mode
    if event.get('test_mode'):
        test_results = test_imports()
        return {
            'statusCode': 200,
            'body': json.dumps({
                'test_mode': True,
                'imports_successful': IMPORTS_SUCCESSFUL,
                'test_results': test_results
            }, default=str)
        }
    
    if not IMPORTS_SUCCESSFUL:
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': 'Critical imports failed',
                'test_results': test_imports()
            }, default=str)
        }
    
    # Check if this is a batch request
    if 'batch_scenarios' in event:
        logger.info(f"ðŸš€ Batch processing mode: {len(event['batch_scenarios'])} scenarios")
        return handle_batch_scenarios(event, context)
    else:
        logger.info("ðŸ“‹ Single scenario processing mode")
        return handle_single_scenario(event, context)


def handle_batch_scenarios(event, context):
    """Handle multiple scenarios in a single Lambda invocation for improved performance."""
    try:
        start_time = time.time()
        batch_scenarios = event['batch_scenarios']
        num_scenarios = len(batch_scenarios)
        
        logger.info(f"ðŸ”„ Processing batch of {num_scenarios} scenarios")
        
        # Get global parameters from event
        num_eval = event.get('num_eval', 1000)
        logger.info(f"ðŸŽ² Monte Carlo simulations: {num_eval} ({'Hikima standard' if num_eval == 1000 else 'custom'})")
        
        runner = PricingExperimentRunner(num_eval=num_eval)
        
        batch_results = []
        
        for i, scenario_event in enumerate(batch_scenarios):
            logger.info(f"ðŸ“‹ Processing scenario {i+1}/{num_scenarios}: {scenario_event.get('scenario_id', f'batch_{i}')}")
            
            # Check remaining time before processing each scenario
            remaining_time = context.get_remaining_time_in_millis() if context else 900000
            if remaining_time < 60000:  # Less than 1 minute remaining
                logger.warning(f"â° Low time remaining ({remaining_time/1000:.1f}s), stopping batch processing")
                batch_results.append({
                    'scenario_id': scenario_event.get('scenario_id', f'batch_{i}'),
                    'status': 'timeout',
                    'error': f'Batch timeout: {remaining_time/1000:.1f}s remaining'
                })
                break
            
            try:
                # Process single scenario
                scenario_result = process_single_scenario_core(scenario_event, runner)
                batch_results.append(scenario_result)
                
            except Exception as e:
                logger.error(f"âŒ Scenario {i+1} failed: {e}")
                batch_results.append({
                    'scenario_id': scenario_event.get('scenario_id', f'batch_{i}'),
                    'status': 'error',
                    'error': str(e)
                })
        
        execution_time = time.time() - start_time
        
        # Return batch results
        return {
            'statusCode': 200,
            'body': json.dumps({
                'batch_mode': True,
                'total_scenarios': num_scenarios,
                'processed_scenarios': len(batch_results),
                'execution_time_seconds': execution_time,
                'results': batch_results
            }, default=str),
            'headers': {'Content-Type': 'application/json'}
        }
        
    except Exception as e:
        logger.error(f"âŒ Batch processing error: {e}")
        logger.error(traceback.format_exc())
        return {
            'statusCode': 500,
            'body': json.dumps({
                'batch_mode': True,
                'error': str(e),
                'traceback': traceback.format_exc()
            }, default=str),
            'headers': {'Content-Type': 'application/json'}
        }


def handle_single_scenario(event, context):
    """Handle a single scenario (original behavior)."""
    try:
        start_time = time.time()
        
        # Get configurable parameters from event (with Hikima defaults)
        num_eval = event.get('num_eval', 1000)  # Default: 1000 (Hikima standard)
        logger.info(f"ðŸŽ² Monte Carlo simulations: {num_eval} ({'Hikima standard' if num_eval == 1000 else 'custom'})")
        
        runner = PricingExperimentRunner(num_eval=num_eval)
        
        # Handle LinUCB training action
        if event.get('action') == 'train_linucb':
            logger.info("ðŸ”§ LinUCB training action requested")
            
            result = runner.train_linucb_model(
                vehicle_type=event.get('vehicle_type', 'green'),
                borough=event.get('borough', 'Manhattan'),
                training_year=event.get('training_year', 2019),
                training_month=event.get('training_month', 7),
                base_price=event.get('base_price', 5.875),
                price_multipliers=event.get('price_multipliers', [0.6, 0.8, 1.0, 1.2, 1.4])
            )
            
            return {
                'statusCode': 200 if result['success'] else 500,
                'body': json.dumps(result, default=str),
                'headers': {'Content-Type': 'application/json'}
            }
        
        # Process single scenario
        scenario_result = process_single_scenario_core(event, runner)
        
        return {
            'statusCode': 200,
            'body': json.dumps(scenario_result, default=str),
            'headers': {'Content-Type': 'application/json'}
        }
        
    except Exception as e:
        logger.error(f"âŒ Single scenario processing error: {e}")
        logger.error(traceback.format_exc())
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e),
                'traceback': traceback.format_exc()
            }, default=str),
            'headers': {'Content-Type': 'application/json'}
        }


def process_single_scenario_core(event, runner):
    """Core scenario processing logic (shared between single and batch modes)."""
    
    start_time = time.time()
    
    # Extract event parameters
    execution_date = event.get('execution_date', datetime.now().strftime('%Y%m%d_%H%M%S'))
    training_id = event.get('training_id', f"{random.randint(100_000_000, 999_999_999)}")
    year = event.get('year', 2019)
    month = event.get('month', 10)
    day = event.get('day', 1)
    time_window = event.get('time_window', {})
    vehicle_type = event.get('vehicle_type', 'green')
    acceptance_function = event.get('acceptance_function', 'PL')
    methods = event.get('methods', ['MinMaxCostFlow'])
    
    logger.info(f"ðŸ§ª Starting pricing experiment: {methods} methods")
    logger.info(f"ðŸ“… Date: {year}-{month:02d}-{day:02d}")
    logger.info(f"ðŸ• Time window: {time_window}")
    logger.info(f"ðŸŽ¯ Acceptance function: {acceptance_function}")
    
    # Parse time window parameters (following Hikima methodology)
    hour_start = time_window.get('hour_start', 10)  # 10:00 AM
    hour_end = time_window.get('hour_end', 20)       # 8:00 PM
    minute_start = time_window.get('minute_start', 0)
    time_interval = time_window.get('time_interval', 5)  # 5 minutes (Hikima standard)
    scenario_index = event.get('scenario_index', 0)
    borough = event.get('borough', 'Manhattan')
    
    # Hikima runs 120 scenarios total (every 5 minutes from 10:00 to 20:00)
    # 10 hours Ã— 12 scenarios per hour = 120 scenarios
    total_scenarios = ((hour_end - hour_start) * 60) // time_interval
    
    logger.info(f"ðŸ“‹ Hikima setup: Running scenario {scenario_index}/{total_scenarios-1} (120 total expected)")
    
    # Calculate specific time window for this scenario (Hikima methodology)
    scenario_minute = scenario_index * time_interval
    current_hour = hour_start + (scenario_minute // 60)
    current_minute = minute_start + (scenario_minute % 60)
    
    # Validate scenario index
    if scenario_index >= total_scenarios:
        logger.warning(f"âš ï¸ Scenario index {scenario_index} exceeds total scenarios {total_scenarios}")
        # Handle out-of-range scenarios gracefully
        scenario_index = scenario_index % total_scenarios
        scenario_minute = scenario_index * time_interval
        current_hour = hour_start + (scenario_minute // 60)
        current_minute = minute_start + (scenario_minute % 60)
    
    # Adjust if minute overflows (shouldn't happen with 5-minute intervals)
    if current_minute >= 60:
        current_hour += current_minute // 60
        current_minute = current_minute % 60
    
    time_start = datetime(year, month, day, current_hour, current_minute, 0)
    time_end = time_start + timedelta(minutes=time_interval)
    
    logger.info(f"â° Hikima time window: {time_start.strftime('%H:%M')} - {time_end.strftime('%H:%M')} (scenario {scenario_index})")
    
    # Validate that we haven't exceeded the day
    if current_hour >= 24:
        logger.error(f"âŒ Invalid time calculation: hour={current_hour} for scenario {scenario_index}")
        return {
            'scenario_id': event.get('scenario_id', f'scenario_{scenario_index}'),
            'status': 'error',
            'error': f'Invalid scenario index {scenario_index}. Max scenarios per day: {total_scenarios}'
        }
    
    # Load real data from S3
    requesters_df, taxis_df = runner.load_taxi_data(
        taxi_type=vehicle_type, 
        year=year, 
        month=month, 
        day=day,
        borough=borough,
        time_start=time_start, 
        time_end=time_end
    )
    
    # Calculate distance matrix and edge weights using Hikima methodology
    distance_matrix, edge_weights = runner.calculate_distance_matrix_and_edge_weights(requesters_df, taxis_df)
    
    logger.info(f"ðŸ“Š Data: {len(requesters_df)} requesters, {len(taxis_df)} taxis")
    
    # Run pricing methods
    results = []
    for method in methods:
        logger.info(f"ðŸ”§ Running method: {method}")
        
        try:
            if method == 'MinMaxCostFlow':
                result = runner.run_minmaxcostflow(requesters_df, taxis_df, distance_matrix, edge_weights, acceptance_function)
            elif method == 'MAPS':
                result = runner.run_maps(requesters_df, taxis_df, distance_matrix, edge_weights, acceptance_function)
            elif method == 'LinUCB':
                result = runner.run_linucb(requesters_df, taxis_df, distance_matrix, edge_weights, acceptance_function, borough, vehicle_type, current_hour)
            elif method == 'LP':
                result = runner.run_lp(requesters_df, taxis_df, distance_matrix, edge_weights, acceptance_function)
            else:
                logger.warning(f"âš ï¸ Unknown method: {method}")
                continue
            
            results.append(result)
            logger.info(f"âœ… {method}: Objective={result['objective_value']:.2f}")
            
        except Exception as e:
            logger.error(f"âŒ {method} failed: {e}")
            results.append({
                'method_name': method,
                'objective_value': 0.0,
                'computation_time': 0.0,
                'error': str(e)
            })
    
    # Calculate comprehensive statistics
    total_objective = sum(r.get('objective_value', 0) for r in results)
    total_computation_time = sum(r.get('computation_time', 0) for r in results)
    avg_computation_time = total_computation_time / len(results) if results else 0
    
    # Data statistics
    data_stats = {
        'num_requesters': len(requesters_df),
        'num_taxis': len(taxis_df),
        'ratio_requests_to_taxis': len(requesters_df) / max(1, len(taxis_df)),
        'avg_trip_distance_km': float(requesters_df['trip_distance_km'].mean()) if len(requesters_df) > 0 else 0,
        'avg_trip_amount': float(requesters_df['total_amount'].mean()) if len(requesters_df) > 0 else 0,
        'avg_trip_duration_seconds': float(requesters_df['trip_duration_seconds'].mean()) if len(requesters_df) > 0 else 0
    }
    
    # Method performance summary
    method_summary = {}
    for result in results:
        method_name = result.get('method_name', 'Unknown')
        method_summary[method_name] = {
            'objective_value': result.get('objective_value', 0),
            'computation_time': result.get('computation_time', 0),
            'success': 'error' not in result
        }
    
    # Save results to S3 only if not disabled
    s3_location = ""
    if not event.get('skip_s3_save', False):
        s3_location = runner.save_results_to_s3(results, event, data_stats, {
            'total_objective_value': total_objective,
            'total_computation_time': total_computation_time,
            'avg_computation_time': avg_computation_time,
            'methods': method_summary
        })
    
    # Prepare final response
    execution_time = time.time() - start_time
    response_data = {
        'scenario_id': event.get('scenario_id', f'scenario_{scenario_index}'),
        'training_id': training_id,
        'execution_date': execution_date,
        'timestamp': datetime.now().isoformat(),
        'status': 'success',
        'execution_time_seconds': execution_time,
        'results': results,
        's3_location': s3_location,
        'experiment_metadata': {
            'year': year,
            'month': month,
            'day': day,
            'borough': borough,
            'time_window': {
                'start': time_start.isoformat(),
                'end': time_end.isoformat(),
                'duration_minutes': time_interval,
                'scenario_index': scenario_index
            },
            'vehicle_type': vehicle_type,
            'acceptance_function': acceptance_function,
            'methods': methods
        },
        'data_statistics': data_stats,
        'performance_summary': {
            'total_objective_value': total_objective,
            'total_computation_time': total_computation_time,
            'avg_computation_time': avg_computation_time,
            'methods': method_summary
        }
    }
    
    logger.info(f"âœ… Experiment completed in {execution_time:.2f}s")
    logger.info(f"ðŸ“Š Results: {len(results)} methods, saved to {s3_location}")
    
    return response_data 