"""Enhanced results aggregator with proper wall-clock time calculation and ranking."""

import json
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

from ..core import get_logger


class EnhancedResultsAggregator:
    """Enhanced aggregator with correct timing, proper statistics, and method ranking."""
    
    def __init__(self, experiment_id: str, config: Dict[str, Any]):
        """Initialize aggregator."""
        self.experiment_id = experiment_id
        self.config = config
        self.logger = get_logger('enhanced_aggregator')
        
        # Track experiment timing - CRITICAL FIX
        self.experiment_start_time = datetime.now()
        self.experiment_end_time = None
        
        # Storage for results
        self.results = []
        self.decisions_data = []
        
        self.logger.debug(f"Enhanced aggregator initialized for {experiment_id}")
    
    def add_result(self, result: Dict[str, Any]):
        """Add a scenario result."""
        self.results.append(result)
    
    def add_decisions_data(self, decisions_df: pd.DataFrame):
        """Add decisions data."""
        if not decisions_df.empty:
            self.decisions_data.append(decisions_df)
    
    def _calculate_stats_without_sum(self, values: List[float]) -> Dict[str, float]:
        """Calculate statistics for ratio values (no sum for ratios)."""
        if not values:
            return {'mean': 0.0, 'median': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}
        
        return {
            'mean': float(np.mean(values)),
            'median': float(np.median(values)),
            'std': float(np.std(values)),
            'min': float(np.min(values)),
            'max': float(np.max(values))
            # NO SUM for ratios/percentages!
        }
    
    def _calculate_stats_with_sum(self, values: List[float]) -> Dict[str, float]:
        """Calculate statistics for absolute values (include sum)."""
        if not values:
            return {'mean': 0.0, 'median': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0, 'sum': 0.0}
        
        return {
            'mean': float(np.mean(values)),
            'median': float(np.median(values)),
            'std': float(np.std(values)),
            'min': float(np.min(values)),
            'max': float(np.max(values)),
            'sum': float(np.sum(values))
        }
    
    def _rank_methods(self, method_stats: Dict[str, Dict]) -> Dict[str, Dict]:
        """Rank methods by objective value and efficiency."""
        if not method_stats:
            return method_stats
        
        # Prepare data for ranking
        method_names = list(method_stats.keys())
        
        # Get objective values (higher is better)
        obj_values = []
        for method in method_names:
            obj_mean = method_stats[method]['objective_values']['mean']
            obj_values.append((method, obj_mean))
        
        # Sort by objective value (descending - higher is better)
        obj_ranked = sorted(obj_values, key=lambda x: x[1], reverse=True)
        
        # Get efficiency ratios (closer to 1.0 is better, but higher is generally better)
        eff_values = []
        for method in method_names:
            # Use mean objective value as efficiency proxy (revenue per scenario)
            eff_mean = method_stats[method]['objective_values']['mean']
            eff_values.append((method, eff_mean))
        
        # Sort by efficiency (descending)
        eff_ranked = sorted(eff_values, key=lambda x: x[1], reverse=True)
        
        # Assign ranks
        for i, (method, _) in enumerate(obj_ranked):
            method_stats[method]['relative_performance']['rank_by_objective'] = i + 1
        
        for i, (method, _) in enumerate(eff_ranked):
            method_stats[method]['relative_performance']['rank_by_efficiency'] = i + 1
        
        # Log ranking for terminal output
        self.logger.info("")
        self.logger.info("=" * 60)
        self.logger.info("METHOD PERFORMANCE RANKING")
        self.logger.info("=" * 60)
        
        self.logger.info("üèÜ BY OBJECTIVE VALUE (Revenue):")
        for i, (method, obj_val) in enumerate(obj_ranked):
            self.logger.info(f"  #{i+1}: {method:<15} - ${obj_val:8.2f}")
        
        self.logger.info("")
        self.logger.info("‚ö° BY EFFICIENCY:")
        for i, (method, eff_val) in enumerate(eff_ranked):
            self.logger.info(f"  #{i+1}: {method:<15} - ${eff_val:8.2f}")
        
        self.logger.info("=" * 60)
        
        return method_stats
    
    def create_comprehensive_summary(self) -> Dict[str, Any]:
        """Create comprehensive experiment summary with CORRECT timing and ranking."""
        # Record experiment end time for wall-clock calculation
        self.experiment_end_time = datetime.now()
        
        # Calculate CORRECT total computation time (wall-clock, not sum of parallel times)
        wall_clock_duration = (self.experiment_end_time - self.experiment_start_time).total_seconds()
        
        # Extract all metrics by method
        method_data = {}
        total_scenarios = len(self.results)
        num_simulations = self.config.get('num_iter', 1)
        
        for result in self.results:
            method = result.get('method', 'unknown')
            if method not in method_data:
                method_data[method] = {
                    'objective_values': [],
                    'computation_times': [],
                    'acceptance_rates': [],
                    'num_scenarios': 0
                }
            
            # Collect data for both PL and Sigmoid
            for acc_func in ['PL', 'Sigmoid']:
                if acc_func in result:
                    method_data[method]['objective_values'].append(result[acc_func]['avg_revenue'])
                    method_data[method]['acceptance_rates'].append(result[acc_func]['avg_acceptance_rate'])
            
            # Computation time (per scenario, not per simulation)
            if 'computation_time' in result:
                method_data[method]['computation_times'].append(result['computation_time'])
            
            method_data[method]['num_scenarios'] += 1
        
        # Build method statistics with proper ranking
        method_stats = {}
        for method, data in method_data.items():
            # Calculate per-simulation time
            total_method_time = np.sum(data['computation_times']) if data['computation_times'] else 0
            total_simulations = data['num_scenarios'] * num_simulations * 2  # *2 for PL and Sigmoid
            per_sim_time_ms = (total_method_time * 1000 / total_simulations) if total_simulations > 0 else 0
            
            method_stats[method] = {
                'objective_values': self._calculate_stats_with_sum(data['objective_values']),
                'computation_times': {
                    **self._calculate_stats_without_sum(data['computation_times']),
                    'total_scenarios_time': float(np.sum(data['computation_times'])) if data['computation_times'] else 0.0,
                    'per_simulation_time_ms': float(per_sim_time_ms),
                    'total_simulations': int(total_simulations)
                },
                'acceptance_rates': self._calculate_stats_without_sum(data['acceptance_rates']),
                'scenarios_completed': data['num_scenarios'],
                'success_rate': 1.0,  # All completed successfully
                'relative_performance': {
                    'rank_by_objective': 1,  # Will be updated by ranking
                    'rank_by_efficiency': 1   # Will be updated by ranking
                }
            }
        
        # Rank methods correctly
        method_stats = self._rank_methods(method_stats)
        
        # Overall performance (across all methods)
        all_obj_values = []
        all_comp_times = []
        all_acc_rates = []
        
        for data in method_data.values():
            all_obj_values.extend(data['objective_values'])
            all_comp_times.extend(data['computation_times'])
            all_acc_rates.extend(data['acceptance_rates'])
        
        # Calculate total simulations for per-simulation timing
        total_simulations_all = total_scenarios * num_simulations * 2  # *2 for PL and Sigmoid
        per_sim_time_ms_overall = (wall_clock_duration * 1000 / total_simulations_all) if total_simulations_all > 0 else 0
        
        # Log timing correction
        if all_comp_times:
            incorrect_sum = np.sum(all_comp_times)
            self.logger.info(f"Timing correction: Wrong sum={incorrect_sum:.1f}s, Correct wall-clock={wall_clock_duration:.1f}s")
            self.logger.info(f"Per simulation time: {per_sim_time_ms_overall:.2f}ms ({total_simulations_all} total simulations)")
        
        return {
            'experiment_metadata': {
                'experiment_id': self.experiment_id,
                'total_scenarios': total_scenarios,
                'wall_clock_duration_seconds': wall_clock_duration,
                'total_simulations': total_simulations_all,
                'experiment_start_time': self.experiment_start_time.isoformat(),
                'experiment_end_time': self.experiment_end_time.isoformat(),
            },
            'experiment_setup': {
                'methods': list(method_data.keys()),
                'hour_start': self.config.get('start_hour', 0),
                'hour_end': self.config.get('end_hour', 0),
                'time_interval': self.config.get('time_delta', 5),  # Add time_interval from time_delta
                'time_unit': self.config.get('time_unit', 'm'),
                'time_window_size': self.config.get('time_window_size', 30),
                'num_eval': num_simulations,
                'num_workers': self.config.get('num_workers', 1),
                'execution_date': str(self.config.get('processing_date', '')).replace('-', ''),
            },
            'results_basic_analysis': {
                'total_methods': len(method_stats),
                'methods_performance': method_stats,
                'overall_performance': {
                    'objective_values': self._calculate_stats_with_sum(all_obj_values),
                    'computation_times': {
                        **self._calculate_stats_without_sum(all_comp_times),
                        'wall_clock_total_seconds': wall_clock_duration,
                        'per_simulation_time_ms': float(per_sim_time_ms_overall),
                        'total_simulations': int(total_simulations_all)
                    },
                    'acceptance_rates': self._calculate_stats_without_sum(all_acc_rates)
                }
            },
            'status': 'completed'
        }
    
    def save_summary(self, summary_file: Path):
        """Save summary to file."""
        summary = self.create_comprehensive_summary()
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        self.logger.info(f"Enhanced summary saved to {summary_file}")